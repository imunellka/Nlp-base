{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3UJF3O-sTNP"
      },
      "outputs": [],
      "source": [
        "# !pip -q install catalyst==20.10.1 transformers datasets nlpaug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYdz8HMxsTNU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from catalyst.utils import set_global_seed, get_device\n",
        "\n",
        "set_global_seed(42)\n",
        "device = \"cuda:0\"\n",
        "# device = get_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VY41eSU8sTNV"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "imdb_dataset = load_dataset(\"imdb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHVyda6AsTNW"
      },
      "outputs": [],
      "source": [
        "imdb_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yz7rBkkcsTNW"
      },
      "outputs": [],
      "source": [
        "imdb_dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCsHKVONsTNW"
      },
      "outputs": [],
      "source": [
        "test = imdb_dataset[\"train\"][0][\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Dl4al6qsTNW"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"google/bert_uncased_L-6_H-256_A-4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-VkvRz_sTNX"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.tokenize(test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SExqTdvusTNX"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.encode(test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZRTacyNsTNY"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.encode_plus(test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQ3RoWThsTNY"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.encode_plus(test, max_length=64, truncation=True, padding=\"max_length\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8g5Ad8hCsTNY"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.encode_plus(test, max_length=64, truncation=True, padding=\"max_length\", return_tensors=\"pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASvbdmKjsTNZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from catalyst.utils import get_loader\n",
        "\n",
        "\n",
        "def text_data_transforms(row):\n",
        "    tokens = tokenizer.encode_plus(row[\"text\"],\n",
        "                                   max_length=64,\n",
        "                                   truncation=True,\n",
        "                                   padding=\"max_length\",\n",
        "                                   return_tensors=\"pt\")\n",
        "    tokens = {k: v[0] for k, v in tokens.items()}\n",
        "    tokens.update({\"targets\": row[\"label\"]})\n",
        "    return tokens\n",
        "\n",
        "\n",
        "train_dataloader = get_loader(\n",
        "    imdb_dataset[\"train\"],\n",
        "    open_fn=lambda x: x,\n",
        "    dict_transform=text_data_transforms,\n",
        "    batch_size=256,\n",
        "    num_workers=4,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "valid_dataloader = get_loader(\n",
        "    imdb_dataset[\"test\"],\n",
        "    open_fn=lambda x: x,\n",
        "    dict_transform=text_data_transforms,\n",
        "    batch_size=256,\n",
        "    num_workers=4,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_02ZhGasTNZ"
      },
      "outputs": [],
      "source": [
        "loaders = {\n",
        "    \"train\": train_dataloader,\n",
        "    \"valid\": valid_dataloader\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xuS14J_sTNZ"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"google/bert_uncased_L-6_H-256_A-4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEiGAC7UsTNa"
      },
      "outputs": [],
      "source": [
        "from catalyst.contrib.nn import RAdam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "\n",
        "optimizer = RAdam(model.parameters(), lr=2e-4)\n",
        "criterion = CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCX3jceysTNa"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "logdir=Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZun4bVKsTNa"
      },
      "outputs": [],
      "source": [
        "from catalyst.dl import SupervisedRunner\n",
        "\n",
        "\n",
        "class BertRunner(SupervisedRunner):\n",
        "    def _handle_batch(self, batch):\n",
        "        self.input = batch\n",
        "        self.output = self.model(**{k: batch[k] for k in self.input_key}, return_dict=True)\n",
        "\n",
        "\n",
        "runner = BertRunner(input_key=[\"input_ids\", \"attention_mask\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epXIrZonsTNa"
      },
      "outputs": [],
      "source": [
        "from catalyst.dl import AccuracyCallback\n",
        "\n",
        "\n",
        "runner.train(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    loaders=loaders,\n",
        "    logdir=logdir,\n",
        "    num_epochs=3,\n",
        "    verbose=True,\n",
        "    callbacks=[AccuracyCallback(num_classes=2)],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text augmentation"
      ],
      "metadata": {
        "id": "f1PO-l-WGitS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcB06mc5sTNb"
      },
      "outputs": [],
      "source": [
        "import nlpaug.augmenter.char as nac\n",
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "text = 'The quick brown fox jumps over the lazy dog .'\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqUD2T2MsTNb"
      },
      "outputs": [],
      "source": [
        "aug = nac.KeyboardAug()\n",
        "augmented_text = aug.augment(text)\n",
        "print(\"Original:\")\n",
        "print(text)\n",
        "print(\"Augmented Text:\")\n",
        "print(augmented_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5yhZ3l5sTNc"
      },
      "outputs": [],
      "source": [
        "aug = naw.SynonymAug(action=\"substitute\")\n",
        "augmented_text = aug.augment(text)\n",
        "print(\"Original:\")\n",
        "print(text)\n",
        "print(\"Augmented Text:\")\n",
        "print(augmented_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYzb2UQdsTNd"
      },
      "outputs": [],
      "source": [
        "aug = naw.ContextualWordEmbsAug(\n",
        "    model_path='bert-base-uncased',\n",
        "    action=\"substitute\"\n",
        ")\n",
        "augmented_text = aug.augment(text)\n",
        "print(\"Original:\")\n",
        "print(text)\n",
        "print(\"Augmented Text:\")\n",
        "print(augmented_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgnYTJR0sTNd"
      },
      "outputs": [],
      "source": [
        "aug = naw.ContextualWordEmbsAug(model_path='google/bert_uncased_L-2_H-128_A-2', action=\"substitute\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acnbvfpFsTNe"
      },
      "outputs": [],
      "source": [
        "def aug_text_data_transforms(row):\n",
        "    sentence = aug.augment(row[\"text\"][:256])# можно потом поставить больше\n",
        "    tokens = tokenizer.encode_plus(sentence,\n",
        "                                   max_length=64,\n",
        "                                   truncation=True,\n",
        "                                   padding=\"max_length\",\n",
        "                                   return_tensors=\"pt\")\n",
        "    tokens = {k: v[0] for k, v in tokens.items()}\n",
        "    tokens.update({\"targets\": row[\"label\"]})\n",
        "    return tokens\n",
        "\n",
        "\n",
        "aug_train_dataloader = get_loader(\n",
        "    imdb_dataset[\"train\"],\n",
        "    open_fn=lambda x: x,\n",
        "    dict_transform=aug_text_data_transforms,\n",
        "    batch_size=256,\n",
        "    num_workers=32,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QOkLVqisTNf"
      },
      "outputs": [],
      "source": [
        "aug_loaders = {\n",
        "    \"train\": aug_train_dataloader,\n",
        "    \"valid\": valid_dataloader\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sllc0xwasTNf"
      },
      "outputs": [],
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"google/bert_uncased_L-6_H-256_A-4\")\n",
        "optimizer = RAdam(model.parameters(), lr=2e-4)\n",
        "criterion = CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8SlftvssTNf"
      },
      "outputs": [],
      "source": [
        "logdir=Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "runner = BertRunner(input_key=[\"input_ids\", \"attention_mask\"])\n",
        "runner.train(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    loaders=aug_loaders,\n",
        "    logdir=logdir,\n",
        "    num_epochs=3,\n",
        "    verbose=True,\n",
        "    callbacks=[AccuracyCallback(num_classes=2)],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03YFTNjtsTNg"
      },
      "source": [
        "### Domain adaptation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqY3ny6BsTNg"
      },
      "outputs": [],
      "source": [
        "sst_dataset = load_dataset(\"glue\", \"sst2\", split=\"train[:10%]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1Ak0nlYsTNg"
      },
      "outputs": [],
      "source": [
        "sst_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61jzveKYsTNh"
      },
      "outputs": [],
      "source": [
        "def sst_text_data_transforms(row):\n",
        "    tokens = tokenizer.encode_plus(row[\"text\"],\n",
        "                                   max_length=64,\n",
        "                                   truncation=True,\n",
        "                                   padding=\"max_length\",\n",
        "                                   return_tensors=\"pt\")\n",
        "    tokens = {k: v[0] for k, v in tokens.items()}\n",
        "    tokens.update({\"targets\": row[\"label\"]})\n",
        "    return tokens\n",
        "\n",
        "sst_train_dataloader = get_loader(\n",
        "    sst_dataset,\n",
        "    open_fn=lambda x: x,\n",
        "    dict_transform=sst_text_data_transforms,\n",
        "    batch_size=256,\n",
        "    num_workers=4,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "\n",
        "sst_loaders = {\n",
        "    \"train\": sst_train_dataloader,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKi5stv3sTNh"
      },
      "outputs": [],
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"google/bert_uncased_L-6_H-256_A-4\")\n",
        "optimizer = RAdam(model.parameters(), lr=2e-4)\n",
        "criterion = CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIStTff7sTNh"
      },
      "outputs": [],
      "source": [
        "logdir=Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "runner = BertRunner(input_key=[\"input_ids\", \"attention_mask\"])\n",
        "runner.train(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    loaders=sst_loaders,\n",
        "    logdir=logdir,\n",
        "    num_epochs=3,\n",
        "    verbose=True,\n",
        "    callbacks=[AccuracyCallback(num_classes=2)],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubtt02c_sTNi"
      },
      "outputs": [],
      "source": [
        "runner.train(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    loaders=loaders,\n",
        "    logdir=logdir,\n",
        "    num_epochs=3,\n",
        "    verbose=True,\n",
        "    callbacks=[AccuracyCallback(num_classes=2)],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEto2vY0sTNi"
      },
      "outputs": [],
      "source": [
        "sst_dataset = load_dataset(\"glue\", \"sst2\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqHtPTIGsTNi"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "\n",
        "model = BertModel.from_pretrained(\"google/bert_uncased_L-2_H-256_A-4\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ar8_w3qLsTNi"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "imdb_vectors = []\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for row in tqdm(imdb_dataset[\"train\"]):\n",
        "        row = text_data_transforms(row)\n",
        "        vector = model(\n",
        "            input_ids=row[\"input_ids\"].unsqueeze(0).to(device),\n",
        "            attention_mask=row[\"attention_mask\"].unsqueeze(0).to(device)\n",
        "        )[0][0, 0].cpu().numpy()\n",
        "        imdb_vectors.append(vector)\n",
        "\n",
        "imdb_vectors = np.array(imdb_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8TjNroVsTNi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "imdb_vectors = np.array(imdb_vectors)\n",
        "imdb_vectors_norm = imdb_vectors/np.linalg.norm(imdb_vectors,axis=1, keepdims=True) # normalize vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_NnbkWbsTNj"
      },
      "outputs": [],
      "source": [
        "sst_scores = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for row in tqdm(sst_dataset):\n",
        "        row = sst_text_data_transforms(row)\n",
        "        vector = model(\n",
        "            input_ids=row[\"input_ids\"].unsqueeze(0).to(device),\n",
        "            attention_mask=row[\"attention_mask\"].unsqueeze(0).to(device)\n",
        "        )[0][0, 0].cpu().numpy()\n",
        "        sst_scores.append(imdb_vectors_norm @ vector /np.linalg.norm(vector)) # calculate cosine metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IDY5xp4sTNj"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.hist(sst_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niI4NElHsTNj"
      },
      "outputs": [],
      "source": [
        "thr = 0.66\n",
        "\n",
        "indeces = [i for i, value in enumerate(sst_scores) if value > thr]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sg4Wxi5xsTNj"
      },
      "outputs": [],
      "source": [
        "sst_train_dataloader = get_loader(\n",
        "    sst_dataset.select(indeces),\n",
        "    open_fn=lambda x: x,\n",
        "    dict_transform=sst_text_data_transforms,\n",
        "    batch_size=256,\n",
        "    num_workers=4,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "\n",
        "sst_loaders = {\n",
        "    \"train\": sst_train_dataloader,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvIOaB3hsTNk"
      },
      "outputs": [],
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"google/bert_uncased_L-6_H-256_A-4\")\n",
        "optimizer = RAdam(model.parameters(), lr=2e-4)\n",
        "criterion = CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPU2XRRPsTNk"
      },
      "outputs": [],
      "source": [
        "logdir=Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "runner = BertRunner(input_key=[\"input_ids\", \"attention_mask\"])\n",
        "runner.train(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    loaders=sst_loaders,\n",
        "    logdir=logdir,\n",
        "    num_epochs=3,\n",
        "    verbose=True,\n",
        "    callbacks=[AccuracyCallback(num_classes=2)],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCwq3fU3sTNk"
      },
      "outputs": [],
      "source": [
        "runner.train(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    loaders=loaders,\n",
        "    logdir=logdir,\n",
        "    num_epochs=3,\n",
        "    verbose=True,\n",
        "    callbacks=[AccuracyCallback(num_classes=2)],\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}